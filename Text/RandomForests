Random Forest Classifier:

Based on the type of the problem we are trying to solve, we presumed that the Random Forest Classifier is a viable method to do so. Among other benefits of this method, it minimises the problem of overfitting, and can help highlight the most relevant features of the areas under study.

The algorithm used for Random Forest Classifier is as follows:

1. For b = 1 to B:
    (a) Draw a bootstrap sample $Z^*$ of size N from the training data.
    (b) Grow a random-forest tree $T_b$ to the bootstrapped data, by 
    recursively repeating the following steps for each terminal node of
    the tree, until the minimum node size $n_{min}$ is reached.
        i. Select m variables at random from the p variables.
        ii. Pick the best variable/split-point among the m.
        iii. Split the node into two daughter nodes.
2. Output the ensemble of trees $\{T_b\}_1^B$

We have tried to eliminate the arguments with high correlation rates, and so the input arguments should be rather independent, but since there are only 72 observations available, which should be divided between the training and testing dataset, using Random Forest Classifier would be the best choice.

We have tried to use the physical features of each location to predict the existence or lack of certain microbiome, to find the most important feature. We also attempted to predict certain features of each location based on the concentration of microbiomes in that location. 

Based on the result, it seems that Random Forest Classification is better at predicting the existence or lack of microbiomes given the physical features measured in that location.


