#======================================================================================================================#
# Parameter File
#======================================================================================================================#
# Feed Forward NN, Siamese NN, Triplet NN, XGBoost, Random Forest
# ['ffnn_keras', 'snn_keras', 'tnn_keras', 'xgb', 'rf_main', 'rf_side']
type:           'rf_side'
# Path to data
DataPath:       ['data/ASV_table_mod.csv','data/Metadata_table.tsv'] #'data/ASV_table_mod.csv'
# Output Path - where to save all the files (phg's etc.)
OutputPath:     'output/'
# Specify the seed
RandomSeed:     1
# Size of the test sample
TestSize:       0.2
#======================================================================================================================#
# Neural Network Configuration (ignore it, if you are using xgboost or random forest)
#======================================================================================================================#
# Loss Functions (most of them are from Keras library) to use. We strongly suggest to use only binary, triplet or contrastive
# because others were not tested
# ['binary', 'contrastive', 'triplet', 'mse', 'mae', 'mape', 'msle', 'hinge', 'shinge', 'chinge', 'logcosh',
# 'categorical', 'sparse', 'kullback', 'poisson', 'proximity']
# with siamese network use 'contrastive', with triplet use 'triplet'
Loss:           'binary' #'triplet' #'contrastive' #'binary'
# Weights (as in keras library)
# Random Normal, Xavier Normal, He Normal, Xavier Uniform, He Uniform
# ['norm', 'xnorm','hnorm', 'unif', 'xunif', 'hunif']
Weights:        'norm'
# Choose number of layers (can be any number)
NHiddenLayers:  3
# Activation functions for hidden and output layers
# (['sigmoid', 'tanh', 'relu', 'softmax'])
HiddenFunc:     'relu' #'sigmoid'
OutputFunc:     'sigmoid'
# Number of Neurons for hidden and output layers
NHiddenNeurons: 2000 # 21 # 4  # 30
NOutputNeurons: 2 # classification = 2, Regression = 1
# Epochs to train the algorithm
epochs:         10
# Optimization Algorithm: choose it wisely :)
#['MBGD', 'Adagrad' 'GD'] <= for minibatch, if you choose 1 you will get just stochstic GD
# if you choose simply GD, then it willl ignore batchSize parameter and will use the whole data set
Optimization:   'adagrad' # please use Adagrad for linear regression (as it may crush overwise
# Batch size for Gradient Descent => if 0, will use simple gradient descent
BatchSize:      10 #16 # 1 #10000 <= increase batch size and it will be good
# Learning rate
alpha:          1e-8 # 0.01 # 0.01 #0.0001 #np.logspace(-5, 1, 7)
# Regularisation - parameter used in Dropout layers
lambda:         0.1